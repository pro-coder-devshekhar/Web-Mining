{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19be5c1c",
   "metadata": {},
   "source": [
    "### Name- Devshekhar Pattnaik\n",
    "\n",
    "### Reg No-19BCE1292\n",
    "\n",
    "### Slot-L19-20\n",
    "\n",
    "\n",
    "# <center> Web Mining Lab-6 </center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101a00e",
   "metadata": {},
   "source": [
    "<b>Q. Take any text corpora, apply necessary preprocessing and perform the k-means clustering on the corpora.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e48b494",
   "metadata": {},
   "source": [
    "<u>Dataset Used:</u>\n",
    "\n",
    "\n",
    "> A Million News Headlines\n",
    "<br>\n",
    "https://www.kaggle.com/therohk/million-headlines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f21fa2",
   "metadata": {},
   "source": [
    "<u>Algorithm</u>\n",
    "\n",
    "\n",
    "1. Import all necessary libraries(numpy,pandas,seaborn etc) required for performing k-means clustering\n",
    "2. Load the dataset and extract all the entries of headline_text column and perform preprocessing by removing duplicate entries\n",
    "3. Select the number K to decide the number of clusters(in this case take 3,4 clusters)\n",
    "4. Select random K points or centroids.\n",
    "5. Assign each data point to their closest centroid, which will form the predefined K clusters\n",
    "6. Calculate the variance and place a new centroid of each cluster\n",
    "7. Repeat step 5, which means reassign each datapoint to the new closest centroid of each cluster\n",
    "8. If any reassignment occurs, then go to step 6 else go to FINISH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221f3773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d228216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"abcnews-date-text.csv\",error_bad_lines=False,usecols =[\"headline_text\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7fbf9",
   "metadata": {},
   "source": [
    "<br><b>Preprocessing</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea259c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116304</th>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57973</th>\n",
       "      <td>10 killed in pakistan bus crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676588</th>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673123</th>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748887</th>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912413</th>\n",
       "      <td>110 with barry nicholls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898238</th>\n",
       "      <td>110 with barry nicholls episode 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827356</th>\n",
       "      <td>110 with barry nicholls episode 15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             headline_text\n",
       "116304     10 killed in pakistan bus crash\n",
       "57973      10 killed in pakistan bus crash\n",
       "676588             110 with barry nicholls\n",
       "673123             110 with barry nicholls\n",
       "748887             110 with barry nicholls\n",
       "912413             110 with barry nicholls\n",
       "898238  110 with barry nicholls episode 15\n",
       "827356  110 with barry nicholls episode 15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebd3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates('headline_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "259be0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "desc = data['headline_text'].values\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d33edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95709\n",
      "['76000', '760000', '760k', '763', '769', '76ers', '76k', '76m', '76pc', '76yo', '76yos', '77', '770', '7700', '770000', '770k', '770m', '771m', '772m', '774', '774s', '775', '775m', '776m', '777', '777s', '779', '77b', '77k', '77kg', '77m', '77pc', '77yo', '78', '78000', '780k', '780m', '783', '783s', '785', '785m', '786', '787', '787s', '78b', '78er', '78ers', '78k', '78kg', '78kph', '78m', '78pc', '78th', '78yo', '79', '7900', '7917', '795', '795pc', '796m', '79b', '79kg', '79m', '79pc', '79th', '79yo', '7b', '7bn', '7cm', '7d', '7eleven', '7ho', '7k', '7kg', '7km', '7m', '7pc', '7pm', '7pmm', '7rar', '7s', '7th', '7wks', '7yo', '7yos', '7yr', '7yrs', '80', '800', '8000', '80000', '800000', '8003', '800b', '800k', '800kg', '800km', '800kms', '800m', '800pc']\n"
     ]
    }
   ],
   "source": [
    "word_features = vectorizer.get_feature_names()\n",
    "print(len(word_features))\n",
    "print(word_features[5000:5100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e69fadfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b141c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEVSHEKHAR\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64035\n",
      "[\"'a\", \"'i\", \"'s\", \"'t\", 'aa', 'aaa', 'aaahhh', 'aac', 'aacc', 'aaco', 'aacta', 'aad', 'aadmi', 'aag', 'aagaard', 'aagard', 'aah', 'aalto', 'aam', 'aamer', 'aami', 'aamodt', 'aandahl', 'aant', 'aap', 'aapa', 'aapt', 'aar', 'aaradhna', 'aardman', 'aardvark', 'aargau', 'aaron', 'aaronpaul', 'aarwun', 'aat', 'ab', 'aba', 'abaaoud', 'ababa', 'aback', 'abadi', 'abadon', 'abal', 'abalon', 'abalonv', 'abama', 'abandon', 'abandond', 'abandong']\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\n",
    "X2 = vectorizer2.fit_transform(desc)\n",
    "word_features2 = vectorizer2.get_feature_names()\n",
    "print(len(word_features2))\n",
    "print(word_features2[:50]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0bfe4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X3 = vectorizer3.fit_transform(desc)\n",
    "words = vectorizer3.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498cccf",
   "metadata": {},
   "source": [
    "<br><b>K=3</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f48fb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEVSHEKHAR\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : polic, say, plan, win, council, govt, australia, report, kill, warn, urg, court, fund, water, nsw, australian, death, crash, open, chang, interview, qld, attack, wa, claim\n",
      "1 : new, zealand, law, year, plan, open, council, polic, home, centr, hospit, deal, set, hope, appoint, australia, look, announc, chief, say, govt, minist, south, mayor, welcom\n",
      "2 : man, charg, murder, polic, court, face, jail, assault, stab, death, drug, die, guilti, child, arrest, attack, sex, accus, car, woman, miss, crash, kill, shoot, alleg\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 3, n_init = 20, n_jobs = 1)\n",
    "kmeans.fit(X3)\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156025b3",
   "metadata": {},
   "source": [
    "<br><b>K=4</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "541efa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEVSHEKHAR\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:792: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 1.0 (renaming of 0.25).\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : polic, say, plan, win, council, govt, australia, kill, warn, court, urg, fund, water, nsw, australian, death, crash, open, interview, chang, attack, qld, claim, wa, face\n",
      "1 : man, charg, murder, polic, court, face, jail, assault, stab, death, drug, die, guilti, child, arrest, attack, accus, sex, car, woman, miss, crash, kill, shoot, alleg\n",
      "2 : new, zealand, law, year, plan, open, council, polic, centr, home, hospit, deal, set, hope, australia, appoint, look, announc, chief, govt, say, minist, south, mayor, welcom\n",
      "3 : report, rural, releas, say, reveal, highlight, polic, urg, govt, health, abc, abus, new, nsw, death, talk, card, council, iraq, qld, kill, child, climat, hospit, chang\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters = 4, n_init = 20, n_jobs = 1)\n",
    "kmeans.fit(X3)\n",
    "# We look at 4 the clusters generated by k-means.\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
